{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "#import code\n",
    "\n",
    "#import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Flatten, Activation, BatchNormalization, Dropout, LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 400\n",
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "SEED = 1998  # Set to None for random seed.\n",
    "NUM_EPOCHS = 2\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(SEED)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(SEED)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "    return imgs\n",
    "\n",
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return np.c_[np.rint(gt_imgs), 1 - np.rint(gt_imgs)]\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if labels[idx][0] > 0.5:\n",
    "                l = 1\n",
    "            else:\n",
    "                l = 0\n",
    "            array_labels[j:j+w, i:i+h] = l\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * PIXEL_DEPTH).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "    color_mask[:,:,0] = predicted_img*PIXEL_DEPTH\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, 'RGB').convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, 'RGB').convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_images(path, x=True):\n",
    "        ids = glob.glob(path + \"*.png\")\n",
    "        N = len(ids)\n",
    "        images = np.empty((len(ids), IMG_SIZE, IMG_SIZE, 3 if x else 1))\n",
    "        for i, id_ in enumerate(ids):\n",
    "            if (x):\n",
    "                img = load_img(id_, target_size=(400,400))\n",
    "            else:\n",
    "                img = load_img(id_, target_size=(400,400), color_mode=\"grayscale\")\n",
    "            images[i] = img_to_array(img)/255.\n",
    "        return images\n",
    "\n",
    "data_dir = 'training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/'\n",
    "\n",
    "# Extract it into np arrays.\n",
    "data = load_images(train_data_filename)\n",
    "labels = load_images(train_labels_filename, x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score, recall, precision\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = np.array([self.validation_data[1],self.validation_data[2]])\n",
    "        _val_f1, _val_recall, _val_precision = f1_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\" — val_f1: %f — val_precision: %f — val_recall %f\" % (_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "# Get a concatenation of the prediction and groundtruth for given input file\n",
    "def get_prediction_with_groundtruth(img_prediction):\n",
    "    cimg = concatenate_images(img, img_prediction)\n",
    "    return cimg\n",
    "\n",
    "# Get prediction overlaid on the original image for given input file\n",
    "def get_prediction_with_overlay(img_prediction):\n",
    "    oimg = make_img_overlay(img, img_prediction)\n",
    "    return oimg\n",
    "\n",
    "class threadsafe_iter:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return self.it.__next__()\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "generator = ImageDataGenerator(rotation_range=360.,\n",
    "                       horizontal_flip = True)\n",
    "\n",
    "@threadsafe_generator\n",
    "def train_generator(generator, x, y):\n",
    "    x_generator = generator.flow(x, seed=SEED)\n",
    "    y_generator = generator.flow(y, seed=SEED)\n",
    "    while True:\n",
    "        xi = x_generator.next()\n",
    "        yi = y_generator.next()\n",
    "        yield xi, yi\n",
    "\n",
    "train_gen = train_generator(generator,data,labels)\n",
    "\n",
    "# Fractal Model.\n",
    "def unet_model(data, n_filters=16, dropout=0.5):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "\n",
    "    def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
    "        # first layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"random_uniform\",\n",
    "                   padding=\"same\")(input_tensor)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "        # second layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"random_uniform\",\n",
    "                   padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "        return x\n",
    "\n",
    "    # contracting path\n",
    "    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=5)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "    p1 = Dropout(dropout*0.5)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "\n",
    "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3)\n",
    "\n",
    "    # expansive path\n",
    "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3)\n",
    "\n",
    "    output = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "    model = Model(inputs=[input_img], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              (None, 400, 400, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 400, 400, 64) 4864        image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 400, 400, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 400, 400, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 400, 400, 64) 102464      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 400, 400, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 400, 400, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 200, 200, 64) 0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200, 200, 64) 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 200, 200, 128 73856       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200, 200, 128 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 200, 200, 128 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 200, 200, 128 147584      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200, 200, 128 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 200, 200, 128 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 100, 100, 128 0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100, 100, 128 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 100, 100, 256 295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 100, 100, 256 1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 100, 100, 256 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 100, 100, 256 590080      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 100, 100, 256 1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 100, 100, 256 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 50, 50, 256)  0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 50, 50, 256)  0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 50, 50, 512)  1180160     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 50, 50, 512)  2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 50, 50, 512)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 50, 50, 512)  2359808     leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 50, 50, 512)  2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 50, 50, 512)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 25, 25, 512)  0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 25, 25, 512)  0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 1024) 4719616     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 1024) 4096        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 25, 25, 1024) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 25, 25, 1024) 9438208     leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 25, 25, 1024) 4096        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 25, 25, 1024) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 50, 50, 512)  4719104     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50, 50, 1024) 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 50, 50, 512)  4719104     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 50, 50, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 50, 50, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 50, 50, 512)  2359808     leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 50, 50, 512)  2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 50, 50, 512)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 100, 100, 256 1179904     leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 100, 100, 512 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 100, 100, 256 1179904     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 100, 100, 256 1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 100, 100, 256 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 100, 100, 256 590080      leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 100, 100, 256 1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 100, 100, 256 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 200, 200, 128 295040      leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 200, 200, 256 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 200, 200, 128 295040      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 200, 200, 128 512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 200, 200, 128 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 200, 200, 128 147584      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 200, 200, 128 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 200, 200, 128 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 400, 400, 64) 73792       leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 400, 400, 128 0           conv2d_transpose_4[0][0]         \n",
      "                                                                 leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 400, 400, 64) 73792       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 400, 400, 64) 256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 400, 400, 64) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 400, 400, 64) 36928       leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 400, 400, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 400, 400, 64) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 400, 400, 1)  65          leaky_re_lu_18[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 34,605,505\n",
      "Trainable params: 34,593,729\n",
      "Non-trainable params: 11,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if RESTORE_MODEL:\n",
    "    print(\"Restoring model\")\n",
    "    model = load_model(\"unet_model_v1.h5\")\n",
    "else:\n",
    "    input_img = Input((IMG_SIZE,IMG_SIZE,3), name='image')\n",
    "    model = unet_model(input_img, n_filters=64)\n",
    "    model.compile(optimizer=Adam(), loss=jaccard_distance_loss)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 2302s 1151s/step - loss: 0.2859\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 1291s 646s/step - loss: 0.2794\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_gen, epochs=NUM_EPOCHS, steps_per_epoch=2, verbose=1)\n",
    "#model.fit(data, labels, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=1, verbose=1)\n",
    "model.save(\"unet_model_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
