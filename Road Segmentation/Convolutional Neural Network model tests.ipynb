{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from PIL import Image\n",
    "\n",
    "#import code\n",
    "\n",
    "#import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Flatten, Activation, BatchNormalization, Dropout, LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import backend as K\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 400\n",
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "SEED = 1998  # Set to None for random seed.\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "RESTORE_MODEL = True # If True, restore existing model instead of training a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(SEED)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(SEED)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "    return imgs\n",
    "\n",
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return np.c_[np.rint(gt_imgs), 1 - np.rint(gt_imgs)]\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if labels[idx][0] > 0.5:\n",
    "                l = 1\n",
    "            else:\n",
    "                l = 0\n",
    "            array_labels[j:j+w, i:i+h] = l\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * PIXEL_DEPTH).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "    color_mask[:,:,0] = predicted_img*PIXEL_DEPTH\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, 'RGB').convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, 'RGB').convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_images(path, x=True):\n",
    "        ids = glob.glob(path + \"*.png\")\n",
    "        N = len(ids)\n",
    "        images = np.empty((len(ids), IMG_SIZE, IMG_SIZE, 3 if x else 1))\n",
    "        for i, id_ in enumerate(ids):\n",
    "            if (x):\n",
    "                img = load_img(id_, target_size=(400,400))\n",
    "            else:\n",
    "                img = load_img(id_, target_size=(400,400), color_mode=\"grayscale\")\n",
    "            images[i] = img_to_array(img)/255.\n",
    "        return images\n",
    "\n",
    "data_dir = 'training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/'\n",
    "\n",
    "# Extract it into np arrays.\n",
    "data = load_images(train_data_filename)\n",
    "labels = load_images(train_labels_filename, x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    # Count positive samples.\n",
    "    c1 = np.sum(np.round(np.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = np.sum(np.round(np.clip(y_pred, 0, 1)))\n",
    "    c3 = np.sum(np.round(np.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score, recall, precision\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = np.array(self.validation_data[1])\n",
    "        _val_f1, _val_recall, _val_precision = f1_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\" — val_f1: %f — val_precision: %f — val_recall %f\" % (_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "# Get a concatenation of the prediction and groundtruth for given input file\n",
    "def get_prediction_with_groundtruth(img_prediction):\n",
    "    cimg = concatenate_images(img, img_prediction)\n",
    "    return cimg\n",
    "\n",
    "# Get prediction overlaid on the original image for given input file\n",
    "def get_prediction_with_overlay(img_prediction):\n",
    "    oimg = make_img_overlay(img, img_prediction)\n",
    "    return oimg\n",
    "\n",
    "class threadsafe_iter:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return self.it.__next__()\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "generator = ImageDataGenerator(rotation_range=360.,\n",
    "                       horizontal_flip = True)\n",
    "\n",
    "@threadsafe_generator\n",
    "def train_generator(generator, x, y):\n",
    "    x_generator = generator.flow(x, seed=SEED, batch_size=BATCH_SIZE)\n",
    "    y_generator = generator.flow(y, seed=SEED, batch_size=BATCH_SIZE)\n",
    "    while True:\n",
    "        xi = x_generator.next()\n",
    "        yi = y_generator.next()\n",
    "        yield xi, yi\n",
    "\n",
    "train_gen = train_generator(generator,data,labels)\n",
    "\n",
    "# Fractal Model.\n",
    "def unet_model(data, n_filters=16, dropout=0.5):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "\n",
    "    def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
    "        # first layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"random_uniform\",\n",
    "                   padding=\"same\")(input_tensor)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "        # second layer\n",
    "        x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"random_uniform\",\n",
    "                   padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "        return x\n",
    "\n",
    "    # contracting path\n",
    "    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=5)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "    p1 = Dropout(dropout*0.5)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "\n",
    "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3)\n",
    "\n",
    "    # expansive path\n",
    "    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3)\n",
    "\n",
    "    output = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "    model = Model(inputs=[input_img], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model\n"
     ]
    }
   ],
   "source": [
    "if RESTORE_MODEL:\n",
    "    print(\"Restoring model\")\n",
    "    model = load_model(\"unet_model_v3.h5\", custom_objects={'jaccard_distance_loss': jaccard_distance_loss})\n",
    "else:\n",
    "    input_img = Input((IMG_SIZE,IMG_SIZE,3), name='image')\n",
    "    model = unet_model(input_img, n_filters=64)\n",
    "    model.compile(optimizer=Adam(), loss=jaccard_distance_loss)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 851s 851ms/step - loss: 0.0242 - val_loss: 0.0649\n",
      " — val_f1: 0.847106 — val_precision: 0.920193 — val_recall 0.784774\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0240 - val_loss: 0.0245\n",
      " — val_f1: 0.974242 — val_precision: 0.981273 — val_recall 0.967311\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0231 - val_loss: 0.0237\n",
      " — val_f1: 0.976916 — val_precision: 0.981754 — val_recall 0.972125\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0232 - val_loss: 0.0246\n",
      " — val_f1: 0.974299 — val_precision: 0.979186 — val_recall 0.969461\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0232 - val_loss: 0.0237\n",
      " — val_f1: 0.977040 — val_precision: 0.982450 — val_recall 0.971689\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0231 - val_loss: 0.0234\n",
      " — val_f1: 0.978002 — val_precision: 0.978973 — val_recall 0.977033\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0232 - val_loss: 0.0239\n",
      " — val_f1: 0.976249 — val_precision: 0.980348 — val_recall 0.972184\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      " — val_f1: 0.975248 — val_precision: 0.980497 — val_recall 0.970054\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0231 - val_loss: 0.0233\n",
      " — val_f1: 0.978142 — val_precision: 0.980479 — val_recall 0.975816\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0229 - val_loss: 0.0237\n",
      " — val_f1: 0.976809 — val_precision: 0.979834 — val_recall 0.973803\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0230 - val_loss: 0.0258\n",
      " — val_f1: 0.970590 — val_precision: 0.974753 — val_recall 0.966462\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0241 - val_loss: 0.0229\n",
      " — val_f1: 0.979527 — val_precision: 0.981939 — val_recall 0.977127\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0230 - val_loss: 0.0226\n",
      " — val_f1: 0.980462 — val_precision: 0.983571 — val_recall 0.977373\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0235 - val_loss: 0.0242\n",
      " — val_f1: 0.975844 — val_precision: 0.973735 — val_recall 0.977963\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0230 - val_loss: 0.0229\n",
      " — val_f1: 0.979534 — val_precision: 0.982555 — val_recall 0.976532\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0229 - val_loss: 0.0228\n",
      " — val_f1: 0.979799 — val_precision: 0.980589 — val_recall 0.979010\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0230 - val_loss: 0.0232\n",
      " — val_f1: 0.978626 — val_precision: 0.981334 — val_recall 0.975933\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0230 - val_loss: 0.0232\n",
      " — val_f1: 0.978763 — val_precision: 0.980500 — val_recall 0.977032\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0229 - val_loss: 0.0240\n",
      " — val_f1: 0.975901 — val_precision: 0.980350 — val_recall 0.971493\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0229 - val_loss: 0.0222\n",
      " — val_f1: 0.981660 — val_precision: 0.983752 — val_recall 0.979576\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0229 - val_loss: 0.0229\n",
      " — val_f1: 0.979205 — val_precision: 0.985495 — val_recall 0.972994\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0229 - val_loss: 0.0223\n",
      " — val_f1: 0.981343 — val_precision: 0.983844 — val_recall 0.978855\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0228 - val_loss: 0.0235\n",
      " — val_f1: 0.977609 — val_precision: 0.980432 — val_recall 0.974802\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0227 - val_loss: 0.0230\n",
      " — val_f1: 0.979269 — val_precision: 0.981625 — val_recall 0.976925\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0231 - val_loss: 0.0234\n",
      " — val_f1: 0.977997 — val_precision: 0.981655 — val_recall 0.974365\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0226 - val_loss: 0.0227\n",
      " — val_f1: 0.979937 — val_precision: 0.983973 — val_recall 0.975933\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0226 - val_loss: 0.0228\n",
      " — val_f1: 0.979627 — val_precision: 0.983086 — val_recall 0.976192\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0226 - val_loss: 0.0222\n",
      " — val_f1: 0.981847 — val_precision: 0.983367 — val_recall 0.980332\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0226 - val_loss: 0.0220\n",
      " — val_f1: 0.982268 — val_precision: 0.987536 — val_recall 0.977055\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0227 - val_loss: 0.0228\n",
      " — val_f1: 0.980039 — val_precision: 0.979784 — val_recall 0.980294\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0229 - val_loss: 0.0232\n",
      " — val_f1: 0.978854 — val_precision: 0.977557 — val_recall 0.980155\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0226 - val_loss: 0.0224\n",
      " — val_f1: 0.981241 — val_precision: 0.981897 — val_recall 0.980586\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0225 - val_loss: 0.0222\n",
      " — val_f1: 0.981717 — val_precision: 0.984151 — val_recall 0.979295\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0231 - val_loss: 0.0224\n",
      " — val_f1: 0.980973 — val_precision: 0.981557 — val_recall 0.980390\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0224 - val_loss: 0.0219\n",
      " — val_f1: 0.982718 — val_precision: 0.985539 — val_recall 0.979913\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0224 - val_loss: 0.0225\n",
      " — val_f1: 0.980706 — val_precision: 0.984925 — val_recall 0.976523\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0230 - val_loss: 0.0220\n",
      " — val_f1: 0.982517 — val_precision: 0.982911 — val_recall 0.982123\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0223 - val_loss: 0.0221\n",
      " — val_f1: 0.981956 — val_precision: 0.985522 — val_recall 0.978415\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0224 - val_loss: 0.0222\n",
      " — val_f1: 0.981757 — val_precision: 0.981953 — val_recall 0.981561\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0225 - val_loss: 0.0228\n",
      " — val_f1: 0.979831 — val_precision: 0.981290 — val_recall 0.978377\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0224 - val_loss: 0.0255\n",
      " — val_f1: 0.971562 — val_precision: 0.977261 — val_recall 0.965930\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0224 - val_loss: 0.0232\n",
      " — val_f1: 0.978605 — val_precision: 0.982100 — val_recall 0.975134\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0227 - val_loss: 0.0219\n",
      " — val_f1: 0.982742 — val_precision: 0.983880 — val_recall 0.981607\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0223 - val_loss: 0.0219\n",
      " — val_f1: 0.982847 — val_precision: 0.982130 — val_recall 0.983564\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0223 - val_loss: 0.0217\n",
      " — val_f1: 0.983227 — val_precision: 0.985399 — val_recall 0.981066\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0223 - val_loss: 0.0221\n",
      " — val_f1: 0.981951 — val_precision: 0.983411 — val_recall 0.980496\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0223 - val_loss: 0.0219\n",
      " — val_f1: 0.982622 — val_precision: 0.983540 — val_recall 0.981705\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0222 - val_loss: 0.0218\n",
      " — val_f1: 0.983033 — val_precision: 0.982369 — val_recall 0.983699\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0223 - val_loss: 0.0212\n",
      " — val_f1: 0.985154 — val_precision: 0.986286 — val_recall 0.984024\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 830s 830ms/step - loss: 0.0222 - val_loss: 0.0235\n",
      " — val_f1: 0.977670 — val_precision: 0.980144 — val_recall 0.975210\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_gen, epochs=NUM_EPOCHS, validation_data=(data, labels), steps_per_epoch=1000, verbose=1, callbacks=[metrics])\n",
    "#model.fit(data, labels, validation_split=0.2, epochs=NUM_EPOCHS, batch_size=1, verbose=1)\n",
    "model.save(\"unet_model_v3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.977528899374121, 0.9800018612830256, 0.9750683867218458)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(data)\n",
    "f1_score(y_pred,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 400, 400, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8336102258686318, 0.7397652384817563, 0.95472446713852)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "# assign a label to a patch\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def compute_patches(data):\n",
    "    patch_data = np.empty((data.shape))\n",
    "    print(patch_data.shape)\n",
    "    for l in range(len(data)):\n",
    "        im_label = data[l]\n",
    "        patch_size = 16\n",
    "        for j in range(0, im_label.shape[1], patch_size):\n",
    "            for i in range(0, im_label.shape[0], patch_size):\n",
    "                patch = im_label[i:i + patch_size, j:j + patch_size]\n",
    "                label = patch_to_label(patch)\n",
    "                patch_data[l,i:i + patch_size, j:j + patch_size] = label\n",
    "    return patch_data\n",
    "\n",
    "patch_data = compute_patches(y_pred)\n",
    "f1_score(patch_data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
